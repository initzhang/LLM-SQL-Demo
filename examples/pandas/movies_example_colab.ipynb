{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Queries in DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to call LLMs directly as a UDF in a DuckDB database using [vLLM](https://github.com/vllm-project/vllm) as the inference engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between OpenAI or vLLM with a quantized version of Llama-3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vLLM engine...\n",
      "WARNING 04-25 15:23:16 config.py:169] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-25 15:23:16 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ', speculative_config=None, tokenizer='TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-25 15:23:16 utils.py:608] Found nccl from library /home/ray/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 04-25 15:23:17 selector.py:65] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 04-25 15:23:17 selector.py:33] Using XFormers backend.\n",
      "INFO 04-25 15:23:19 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 04-25 15:23:21 model_runner.py:173] Loading model weights took 5.3472 GB\n",
      "INFO 04-25 15:23:27 gpu_executor.py:119] # GPU blocks: 2602, # CPU blocks: 2048\n",
      "INFO 04-25 15:23:29 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-25 15:23:29 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-25 15:23:41 model_runner.py:1057] Graph capturing finished in 12 secs.\n",
      "INFO 04-25 15:23:41 block_manager_v1.py:235] Automatic prefix caching is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the below code to initialize llmsql with OpenAI\n",
    "# import llmsql\n",
    "# from llmsql.llm.openai import OpenAI\n",
    "\n",
    "\n",
    "# llmsql.init(OpenAI(base_url=\"https://api.openai.com/v1\", api_key=\"<INSERT_OPENAI_KEY>\"))\n",
    "\n",
    "\n",
    "# Uncomment the below code to initialize llmsql with vLLM\n",
    "\n",
    "import llmsql\n",
    "from llmsql.llm.vllm import vLLM\n",
    "from vllm import EngineArgs\n",
    "args = EngineArgs(model=\"TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ\")\n",
    "\n",
    "llmsql.init(vLLM(engine_args=args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the movies dataset as a DuckDB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20 entries, 1 to 26\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   rotten_tomatoes_link  20 non-null     object\n",
      " 1   review_content        20 non-null     object\n",
      " 2   movie_title           20 non-null     object\n",
      " 3   movie_info            20 non-null     object\n",
      " 4   id                    20 non-null     int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 960.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a table from the movies dataset\n",
    "df = pd.read_csv(\"../movies_small.csv\")\n",
    "df = df[df[\"review_content\"].notnull()]\n",
    "df = df[:20]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run sample LLM Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsql.pandas import query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMs in Projection Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:02<00:00,  7.82it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Given a movie review as {review_content}, classify the review as either POSITIVE, NEGATIVE, or NEUTRAL. Respond with just the category and no other text.\"\n",
    "result = query(prompt, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie review: It's a series of routines within a routine formula, and the result is as tedious as it sounds.\n",
      "Sentiment: NEGATIVE\n",
      "\n",
      "Movie review: A vulgar exercise of terror that, despite its defects, manages to stand out from its delectable predecessors. [Full Review in Spanish]\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Movie review: After the Thin Man hasn't quite the spontaneity and charm of the original, but it's good mystery-comedy, the dialogue bright, the handling expert, and the principals as ingratiating as ever.\n",
      "Sentiment: POSITIVE\n",
      "\n",
      "Movie review: You never really get angry at it. You just want to shake it up because the elements for a first-class comedy thriller are all there. It's simply .that everything is always ten per cent off.\n",
      "Sentiment: NEUTRAL\n",
      "\n",
      "Movie review: An excellent film.\n",
      "Sentiment: POSITIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 results\n",
    "for result in list(zip(df[\"review_content\"], result))[:5]:\n",
    "    print(f\"Movie review: {result[0]}\")\n",
    "    print(f\"Sentiment: {result[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/20 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 20/20 [00:04<00:00,  4.81it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Given {movie_title}, {movie_info} and a movie review as {review_content}, extract all character names that are mentioned. \"\n",
    "    \"Respond with just the character names and no other text. If there are no characters, respond with just None\")\n",
    "result = query(prompt, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character names: Jon Hamm\n",
      "Character names: James Coburn, Virginia Madsen\n",
      "Character names: Aaron Eckhart, Bill\n",
      "Character names: Lilyan Chauvin, Gilmer McCormick\n",
      "Character names: Annibal Ramirez, Carlos Sanchez, Jack Shaw, Amos\n"
     ]
    }
   ],
   "source": [
    "for characters in result[:5]:\n",
    "    print(f\"Character names: {characters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the LLM query as filters, possibly in combination with projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:00<00:00, 29.71it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Given a movie review as {review_content}, classify the review as either POSITIVE, NEGATIVE, or NEUTRAL. \"\n",
    "    \"Respond with just the category and no other text.\")\n",
    "result = query(prompt, df)\n",
    "filtered_movies = df[[r==\"POSITIVE\" for r in result]][\"movie_title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amityville: The Awakening\n",
      "After the Thin Man\n",
      "Silver Streak\n",
      "Boomerang!\n",
      "Hoodlum\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 results\n",
    "for result in zip(filtered_movies[:5]):\n",
    "    print(f\"{result[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:00<00:00, 30.23it/s]\n",
      "Processed prompts: 100%|██████████| 6/6 [00:00<00:00, 10.99it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Given a movie review as {review_content}, classify the review as either POSITIVE, NEGATIVE, or NEUTRAL. \"\n",
    "    \"Respond with just the category and no other text.\")\n",
    "result = query(prompt, df)\n",
    "filtered_movies = df[[r==\"POSITIVE\" for r in result]]\n",
    "\n",
    "\n",
    "prompt = \"Given {movie_title} and {movie_info}, determine if this movie is suitable for kids. \"\n",
    "result = query(prompt, filtered_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie title: Amityville: The Awakening\n",
      "Suitable for kids: No, this movie is not suitable for kids.\n",
      "\n",
      "Movie title: After the Thin Man\n",
      "Suitable for kids: unsuitable\n",
      "\n",
      "Movie title: Boomerang!\n",
      "Suitable for kids: unsuitable\n",
      "\n",
      "Movie title: Hoodlum\n",
      "Suitable for kids: No\n",
      "\n",
      "Movie title: Meet Bill\n",
      "Suitable for kids: Not suitable for kids\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print first 5 results\n",
    "for title, kids in list(zip(filtered_movies[\"movie_title\"], result))[:5]:\n",
    "    print(f\"Movie title: {title}\")\n",
    "    print(f\"Suitable for kids: {kids}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM queries can also be used in aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 20/20 [00:00<00:00, 35.40it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Given a movie review as {review_content}, score the movie either as 1, 2, 3, with 3 as the highest. Return just the score and nothing else.\"\n",
    "df[\"review_score\"] = [int(score) for score in query(prompt, df)]\n",
    "grouped_df = df.groupby(\"movie_title\")[\"review_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie_title\n",
       "After the Thin Man            2.0\n",
       "Alexandra's Project           2.0\n",
       "American Gun                  2.0\n",
       "Amityville: The Awakening     2.0\n",
       "Armstrong                     3.0\n",
       "Badland                       2.0\n",
       "Boomerang!                    2.5\n",
       "Death in Love                 2.0\n",
       "Hoodlum                       2.0\n",
       "Meet Bill                     3.0\n",
       "Silent Night, Deadly Night    2.0\n",
       "Silver Streak                 3.0\n",
       "Stolen                        2.0\n",
       "The Assignment                3.0\n",
       "The Jazz Singer               3.0\n",
       "White Fang                    2.0\n",
       "Name: review_score, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
