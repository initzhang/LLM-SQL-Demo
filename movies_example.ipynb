{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Queries in DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to call LLMs directly as a UDF in a DuckDB database using [vLLM](https://github.com/vllm-project/vllm) as the inference engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vLLM engine...\n",
      "WARNING 04-11 12:31:13 config.py:211] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 04-11 12:31:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='TheBloke/Llama-2-13B-chat-GPTQ', tokenizer='TheBloke/Llama-2-13B-chat-GPTQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 04-11 12:31:14 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 04-11 12:31:14 selector.py:25] Using XFormers backend.\n",
      "INFO 04-11 12:31:17 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 04-11 12:31:20 model_runner.py:104] Loading model weights took 6.8127 GB\n",
      "INFO 04-11 12:31:25 gpu_executor.py:94] # GPU blocks: 433, # CPU blocks: 327\n",
      "INFO 04-11 12:31:28 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-11 12:31:28 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 04-11 12:31:44 model_runner.py:867] Graph capturing finished in 16 secs.\n",
      "INFO 04-11 12:31:44 block_manager_v1.py:233] Automatic prefix caching is enabled.\n"
     ]
    }
   ],
   "source": [
    "import llmsql\n",
    "from llmsql.llm.vllm import vLLM\n",
    "from vllm import EngineArgs\n",
    "\n",
    "args = EngineArgs(model=\"TheBloke/Llama-2-13B-chat-GPTQ\")\n",
    "\n",
    "# Initialize llmsql\n",
    "llmsql.init(vLLM(engine_args=args))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the movies dataset as a DuckDB table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x7141999169b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure you import duckdb from llmsql\n",
    "from llmsql.duckdb import duckdb\n",
    "\n",
    "# Create a table from the movies dataset\n",
    "conn = duckdb.connect(database=':memory:', read_only=False)\n",
    "conn.execute(\"CREATE TABLE movies AS SELECT * FROM read_csv('movies_small.csv')\")\n",
    "conn.execute(\"CREATE TABLE movies_small as SELECT * FROM movies LIMIT 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌──────────────┐\n",
      "│     name     │\n",
      "│   varchar    │\n",
      "├──────────────┤\n",
      "│ movies       │\n",
      "│ movies_small │\n",
      "└──────────────┘\n",
      "\n",
      "┌──────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n",
      "│     column_name      │ column_type │  null   │   key   │ default │  extra  │\n",
      "│       varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n",
      "├──────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n",
      "│ rotten_tomatoes_link │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ review_content       │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ movie_title          │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ movie_info           │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "│ id                   │ BIGINT      │ YES     │ NULL    │ NULL    │ NULL    │\n",
      "└──────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the table and fields in the table\n",
    "\n",
    "print(conn.sql(\"SHOW TABLES\"))\n",
    "\n",
    "print(conn.sql(\"DESCRIBE movies_small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the LLM Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT LLM('Summarize the {review_content}. Return just the summary and nothing else.', review_content) FROM movies_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the CachedLlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8446ef869ff4d2d9892d6f39c8cdd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_result = conn.execute(query).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"  Sure! Here's the summary of the review content:\\n\\nNo\",),\n",
       " ('  Sure! Here is the summary of the review content:\\n\\n\"ted',),\n",
       " ('  Sure! Here\\'s the summary of the review content:\\n\\n\"',),\n",
       " ('  Sure! Here\\'s the summary of the review content:\\n\\n\"',),\n",
       " ('  Sure! Here\\'s the summary of the review content:\\n\\n\"',),\n",
       " ('  Sure! Based on the provided data, the summary of the review content is',),\n",
       " (\"  Sure, I'd be happy to help! Based on the data provided\",),\n",
       " (\"  Sure, I'd be happy to help! Here is the summary of\",),\n",
       " ('  Sure! Here\\'s the summary of the review content:\\n\\n\"',),\n",
       " ('  Sure! Based on the provided JSON data, the summary of the review content',)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
